# -*- coding: utf-8 -*-
"""Final_Sep_dbert_sentiment_model_with_recall and cmatrix.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K-AiJAkB8-S9F25g1h1yhj2Rg0pcTdqu
"""

import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

!pip install transformers

import pandas as pd
import numpy as np
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW
import torch
from torch.utils.data import DataLoader, TensorDataset, random_split

from google.colab import files
uploaded = files.upload()

import io
df = pd.read_excel(io.BytesIO(uploaded['data.xlsx']))
df

# Preprocessing
def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'\d+', '', text)   # Remove digits
    text = re.sub(r'[^\w\s]', '', text)   # Remove punctuation
    text = re.sub(r'\s+', ' ', text).strip()   # Remove extra spaces
    return text

df['NPS Comment'] =  df['NPS Comment'].apply(preprocess_text)

# Convert sentiment labels to integers
label_map = {'negative': 0, 'neutral': 1, 'positive': 2}
df['Sentiment'] = df['Sentiment'].map(label_map)

# Tokenize the text using DistilBERT tokenizer
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")

# Convert data to DistilBERT-compatible format (input_ids and attention_masks)
input_ids = []
attention_masks = []
for comment in df['NPS Comment']:
    encoded_dict = tokenizer.encode_plus(comment,
                                         add_special_tokens=True,
                                         max_length=128,  # Adjust as needed based on your dataset
                                         padding='max_length',
                                         truncation=True,
                                         return_attention_mask=True,
                                         return_tensors='pt')
    input_ids.append(encoded_dict['input_ids'])
    attention_masks.append(encoded_dict['attention_mask'])

input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
labels = torch.tensor(df['Sentiment'])

# Create DataLoader
dataset = TensorDataset(input_ids, attention_masks, labels)
train_size = int(0.8 * len(dataset))
train_dataset, test_dataset = random_split(dataset, [train_size, len(dataset) - train_size])
train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Load the pre-trained DistilBERT model
model_name = "distilbert-base-uncased"
model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=3)  # 3 classes - negative, neutral, positive

# Fine-tuning parameters and training
optimizer = AdamW(model.parameters(), lr=2e-5)
loss_fn = torch.nn.CrossEntropyLoss()

epochs = 5
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

for epoch in range(epochs):
    model.train()
    for batch in train_dataloader:
        batch_input_ids, batch_attention_masks, batch_labels = tuple(t.to(device) for t in batch)
        optimizer.zero_grad()
        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_masks, labels=batch_labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

# Evaluation
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for batch in test_dataloader:
        batch_input_ids, batch_attention_masks, batch_labels = tuple(t.to(device) for t in batch)  # Move data to the same device as the model
        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_masks)
        predicted_labels = torch.argmax(outputs.logits, dim=1)
        total += batch_labels.size(0)
        correct += (predicted_labels == batch_labels).sum().item()

accuracy = correct / total
print(f"Test Accuracy: {accuracy}")

# Calculate Precision and Recall
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

model.eval()
all_predicted_labels = []
all_batch_labels = []

with torch.no_grad():
    for batch in test_dataloader:
        batch_input_ids, batch_attention_masks, batch_labels = tuple(t.to(device) for t in batch)
        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_masks)
        predicted_labels = torch.argmax(outputs.logits, dim=1)

        all_predicted_labels.extend(predicted_labels.cpu().numpy())
        all_batch_labels.extend(batch_labels.cpu().numpy())

precision_weighted = precision_score(all_batch_labels, all_predicted_labels, average='weighted')
recall_weighted = recall_score(all_batch_labels, all_predicted_labels, average='weighted')
f1_weighted = f1_score(all_batch_labels, all_predicted_labels, average='weighted')

print(f"Weighted Precision: {precision_weighted}")
print(f"Weighted Recall: {recall_weighted}")
print(f"Weighted F1 Score: {f1_weighted}")

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_batch_labels, all_predicted_labels)

# Define class labels
class_labels = ['Negative', 'Neutral', 'Positive']

# Plot the confusion matrix as a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

# Print the confusion matrix
print("Confusion Matrix:")
print(conf_matrix)
print()

def predict_sentiment(comment):
    inputs = tokenizer.encode_plus(comment,
                                   add_special_tokens=True,
                                   max_length=128,
                                   padding='max_length',
                                   truncation=True,
                                   return_attention_mask=True,
                                   return_tensors='pt')
    input_ids = inputs['input_ids'].to(device)  # Move input_ids to the same device as the model
    attention_mask = inputs['attention_mask'].to(device)  # Move attention_mask to the same device as the model

    model.eval()  # Set the model to evaluation mode
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)

    predicted_label = torch.argmax(outputs.logits, dim=1).item()
    sentiment_mapping = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}
    return sentiment_mapping[predicted_label]

# Test the model with custom comments
test_comments = [
    "CompanyXYZ is unable to solve issues. All 4 vessels in Australia currently have connection issues of some sort, and CompanyXYZ is slow, unresponsive, and unable to provide any solutions.",
    "The customer service was terrible.",
    "Not at all.",
    "Not at all good",
    "The delivery was on time and the product quality was good.",
]

for comment in test_comments:
    sentiment = predict_sentiment(comment)
    print(f"Comment: '{comment}' --> Sentiment: {sentiment}")